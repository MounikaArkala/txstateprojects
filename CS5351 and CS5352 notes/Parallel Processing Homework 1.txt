Parallel Processing Homework 1
by Virpobe Paireepinart
for CS5351, Texas State University
Summer 2009

Problem 1.

a) T(n) = T(n/2) + 2 lg(n)
assume T(1) = c; that is, the time complexity of a single element is constant time.
we can solve this using recurrence relations.
T(n) = T(n/2) + 2lg(n)
     = [T(n/4) + 2lg(n/2)] + 2lg(n)
     = T(n/4) + 2lg(n) + 2lg(n/2)
     = T(n/4) + 2( lg(n) + lg(n/2) )
     = T(n/4) + 2( lg(n*n/2) )
     = [T(n/8) + 2lg(n/4)] + 2( lg(n*n/2) )
     = T(n/8) + 2( lg(n*n/2) + lg(n/4) )
     = T(n/8) + 2( lg(n*n/2*n/4) )
     = [T(n/16) + 2lg(n/8)] + 2( lg(n*n/2*n/4) )
     = T(n/16) + 2( lg(n*n/2*n/4) + lg(n/8) )
     = T(n/16) + 2( lg(n * n/2 * n/4 * n/8) )
     = T(n/16) + 2( lg(n^4 / 2^6) )
     = T(n/32) + 2( lg( n^5 / 2^10) )
     = T(n/64) + 2( lg( n^6 / 2^15) )
     = T(n/(2^6) + 2( lg( n^6 / 2^15) )
 
following this pattern, we get
T(n) = T(n/(2^k)) + 2 lg( (n^k) / 2^(SUM[1...k-1]) )

We want to get rid of T(n/(2^k)) so we need to set k to a value that will eliminate this term.
let k = lg(n) = log2(n)

T(n) = T(n/(2^lg(n)) + 2 lg( (n^lg(n)) / 2^(SUM[1...lg(n)-1]) )

we know 2^log2(n) = n, since logs are reverse exponential notation.
Therefore,
T(n) = T(n/n) + 2 lg( (n^lg(n)) / 2^(SUM[1...lg(n)-1]) )
which simplifies to
T(n) = T(1) + 2 lg( (n^lg(n)) / 2^(SUM[1...lg(n)-1]) )
T(1) has constant runtime, so it does not influence our time complexity analysis and can be removed.

T(n) = 2 lg( (n^lg(n)) / 2^(SUM[1...lg(n)-1]) )
The constant 2 can also be dropped.
we are left with
T(n) = lg( (n^lg(n)) / 2^(SUM[1...lg(n)-1) )
we can expand this back out to
T(n) = lg(n) + lg(n/2) + lg(n/4) + lg(n/8) + ... lg(n/lg(n-1))
this growth is log-logarithmic, so it is bounded above by

O(n) = lg(n)
and below by
Omega(n) = (lg(lg(n)))



b)
T(n) = 4T(n/4) + C
     = 4(4T(n/16)) + C
     = 16T(n/16) + C
This simplifies to
4^k T(n/4^k)
or
2^(k+1) T(n / (2^(k+1)))

We assume k+1 = a to simplify solution.
2^(a) T(n / (2^a))

we can take a to be the same as "k" was in the previous problem,
because our goal is still to eliminate the term T(n / (2^a)) as before.
let a = lg(n) = log2(n)

Then
2^lg(n) T(n/2^lg(n)) = n T(n/n)
                     = n C
                     = n
 Therefore, O(n) = n and Omega(n) = n
I.E. they are linear time complexity.



c)
T(n) = (2/3)T(n/2) + n
     = (2/3)((2/3)T(n/4) + n/2) + n
     = (2/3)^2 T(n/4) + n/3 + n
     = (2/3)^2 ((2/3)T(n/8) + n/4) + n/3 + n
     = (2/3)^3 T(n/8) + n/9 + n/3 + n
     
let C = (2/3)
     = C^3 T(n/8) + C^2n/3 + Cn/2 + n
     = C^k T(n/k) + SUM[0...n-1] (C^k * n / k)
at k=n, we get T(n/n) which = T(1) which is a constant, which can be removed.
C^n + SUM[0...n-1] (C^n * n / n)
once again, (n/n) can be eliminated,
C^n + SUM[0...n-1] (C^n)

we can substitute 2/3 for C...
(2/3)^n + (2/3)^n + (2/3)^n .... (2/3)^n = n((2/3)^n)

Therefore, the O(n) = n and the Omega(n) = n((2/3)^n)


Problem 2.

Problems that the textbook notes, w/ Control-Parallel Sieve of Erasothenes: 

1) two processors may end up using the same prime value to sieve through the array.
Supposing there are two processors, p1 and p2.
p1 can choose 2 as its prime, and before it can mark the "current" prime as 3, p2 can also choose 2 as its prime.
Then they will both be sieving the same prime (which is inefficient.)

2) a processor may end up sieving multiples of a composite number.
p1 gets 2, p2 gets 3, and before p1 can strike out 4 (which is not a prime) p3 gets 4,
so it's sieving on a non-prime.

3) When regarding figure 1-9 in the textbook, there is one main issue that presents itself regarding 
the Control Parallel approach to the Sieve of Erasothenes.
The main issue is that of the primes of multiples of 2.
The idea behind parallelizing algorithms is to allow them to run in a smaller amount of time,
at the cost of more processing power.
However, with the control-parallel approach, there is always a significant lower-bound on reducing the time complexity - 
the time it takes to sieve the number "2".  because a number can only be sieved by a single processor in the
control-parallel approach, the time it takes the first processor to strike out all multiples of "2" is the minimum possible run-time.



However, the textbook mentions all three of these issues, and I could not find any others that didn't have their base
cause rooted in one of these three problems.

For example, take the problem of processor 3 in figure 1-9 item (c).
There is an issue with it choosing prime 4 accidentally, if prime 2 and 3 have not started sieving yet.
Therefore it has to wait a bit for processor 1 and procsesor 2 to execute.  How long does it have to wait?  How can it know?
But this problem is rooted in issue #2 listed above.

Also, since processor 3 has to wait before it can execute, then the more processors you have, the more waiting they have to do in order to not
choose an incorrect prime to sieve.  But this is still just a side-effect of issue #2.

The side-effect of enforcing the issues #1 and #2 is that your algorithm will be slowed down somewhat.  You have to synchronize the updates
(so that if a processor is choosing a prime to sieve, no other processors can also choose one until it has finished its decision (this fixes issue #1))
and you have to make sure primes have been evaluated up to the current position (so in order to choose 4
you have to be sure that primes 2-3 have already been evaluated past 4, etc.  This addresses issue 2.)

If you want to increase the execution speed that is caused by issue #3, you cannot simply add more processors.
Once you get to a certain point, adding more processors will not speed up execution.
(because sieving "2" takes approximately 1/3 of the overall time, 3 is the maximum processors that increase performance.)

This means that this algorithm is not scaleable.
However, this is common for control-parallel solutions.  To get a scaleable algorithm you must move to a Data Parallel approach.


Problem 3.


In the textbook solution to the Sieve of Eratosthenes problem, using the data- parallel approach,
the communication cost is a significant portion of the total cost...
after a certain number of processors are added, the communication cost will surpass the computational gain provided by the extra processors,
and you will not gain speedup.
If processor 1 (the one who is choosing the next prime) can broadcast the next prime to all other processors, then this is a linear time requirement.
Therefore, the communication cost does not ever surpass the computational gain by adding extra processors,
so your speedup is only bounded by the number of primes you are processing -
eventually you will have a processor assigned to every element and adding more
processors will not add any speedup (unless you increase the search space so that there are more elements to analyze.)

Since the communication cost is now fixed, the total cost is now lowered.
Originally, communication cost was based on the number of processors
to transmit to.
That is, communicating k primes to (p-1) processors, with a transmission
time of lambda per-processor, used to be k * (p-1) * lambda.
However, now that only a single transmission needs to take place, the new
time for communication of the next prime is merely k * lambda, which is a fixed cost based solely on the number of
primes and the transmission time ( that is, for a specific instance of the problem, scaling up the number
of processors does not scale up the transmission time at all.)
Therefore, your cost is merely a function of the processor striking-out time, with a fixed communication time.
Then it doesn't exhibit the same symptoms as the textbook-defined version.
There was a limit past which processors wouldn't allow any speedup, because the communication time was increasing linearly.
however, with our broadcast solution, the communication time does not increase linearly with the number of procsesors used.
Therefore, our total time spent is:
( ceil(ceil(n/p) / 2) + ceil(ceil(n/p) / 3) + ceil(ceil(n/p) / 5) + ...
+ ceil(ceil(n/p) / pi_k) ) X  + k * lambda.
where ceil() is the ceiling function (round up to the nearest integer)
n is the maximum number to check for primality,
pi_k is the largest prime less than sqrt(n),
and X is the time units it takes for a processor to mark a multiple of a prime as being a composite number.

So the total time will decrease with each added processor, and as the number of processors approaches infinity,
the runtime approaches k * lambda.

Problem 4.
KRUSKAL's algorithm - pg 690 in Discrete Math book.
The MST (minimum-spanning tree) is an important problem.
We seek to solve this using the CREW PRAM model.  This means a Concurrent-Read, Exclusive-Write, Parallel Random Access Memory model.
An MST is defined as a "[...] tree in a connected weighted graph [...] that has the smallest possible sum of weights of its edges."
We want to find an algorithm that conforms to the CREW PRAM model
and works on a connected unidirectional weighted graph.
"connected" means that all vectors can be reached from all other vectors (though not always directly.) 
"unidirectional" means that each connection can be traversed in either direction.
"weighted" means that each connection in the graph contains a value associated with it, which is the "cost" of traversing that connection.

Connections are referred to as "edges" for simplicity.

There are two common algorithms taught in Discrete Mathematics class relating to MST's: Prim's and Kruskal's.

Both Prim's and Kruskal's algorithms can be applied to this problem.
We will use Kruskal's algorithm to solve the problem.

Kruskal's algorithm can be described in a few steps:
tree = empty
for all edges:
    get smallest weight edge
    if it doesn't form a circuit, add it to our tree
    
The problem here is how to parallelize this.
There are two main tasks - get smallest weight edge, and check if it forms a circuit.
Getting the smallest weight edge can be easily performed by simply sorting the edge values.
This can be performed by a sort function in the PRAM model.
The sorting will not be reproduced here but a PRAM merge sort has time complexity O(n) with unlimited processors.
The second part of the problem, finding whether adding an edge forms a circuit, can be performed in parallel (somewhat).

So the final PRAM algorithm is as such:
tree = empty
PRAM_SORT(edges)
for edge in edges:
    spawn processor for edge
    check if edge forms a circuit if added to graph
    - if it does, immediately eliminate edge.
    - if it doesn't, wait for all previous processors to complete checking.
    - if any previous processors added an edge, recalculate circuit.

This is not a perfect solution, because there is still a case where a process must wait for other processes to complete,
but it is the most efficient way I can think of to solve the problem.
The worst-case time complexity, every process would have to recalculate if its edge formed a circuit for every previous process,
so the runtime would be the same as the non-parallel version, or O(n^k) for k edges and n nodes (because it takes n tries to check each edge for circuit.)

Therefore, the overall time complexity would be O(n) + O(n^k) or O(n + n^k) = O(n^k).